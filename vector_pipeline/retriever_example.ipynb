{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a830265f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.load_config import load_config\n",
    "from vector_store.weaviate import WeaviateCollectionManager\n",
    "from parser.colqwen import Colqwen\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb48da5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73670606",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8754ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(image: Image.Image, new_height: int = 1024) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Scale an image to a new height while maintaining the aspect ratio.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    scaled_image = image.resize((new_width, new_height))\n",
    "\n",
    "    return scaled_image\n",
    "\n",
    "\n",
    "def load_and_scale_image(image_base64: str, new_height: int = 1024) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a base64 string and scale it to the specified height.\n",
    "    \"\"\"\n",
    "    # Decode base64 to bytes and open with PIL\n",
    "    image = decode_base64_to_image(image_base64)\n",
    "    \n",
    "    # Scale the image\n",
    "    scaled_image = scale_image(image, new_height)\n",
    "\n",
    "    return scaled_image\n",
    "\n",
    "def decode_base64_to_image(base64_str: str) -> Image.Image:\n",
    "    image_data = base64.b64decode(base64_str)\n",
    "    return Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def build_flexible_message_payload(\n",
    "    base64_images: List[str],\n",
    "    user_text: str,\n",
    "    system_prompt: str = \"You are an intelligent assistant that summarizes the visual content of images.\",\n",
    "    additional_user_content: List[Dict[str, Union[str, Dict]]] = None\n",
    ") -> List[Dict[str, Union[str, List[Dict[str, Union[str, Dict]]]]]]:\n",
    "    \"\"\"\n",
    "    Build a flexible multimodal message payload for GPT-4o.\n",
    "    \"\"\"\n",
    "    image_payloads = [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{img}\"\n",
    "            }\n",
    "        }\n",
    "        for img in base64_images\n",
    "    ]\n",
    "\n",
    "    text_payload = {\"type\": \"text\", \"text\": user_text}\n",
    "\n",
    "    user_content = image_payloads + [text_payload]\n",
    "\n",
    "    if additional_user_content:\n",
    "        user_content += additional_user_content\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def ask_gpt4o(open_ai_client, messages: List[Dict]) -> str:\n",
    "    response = open_ai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc223b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d4c65",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a19c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "open_ai_key = os.getenv(\"OPENAI_APIKEY\")\n",
    "\n",
    "# Set up OpenAI model\n",
    "open_ai_client = OpenAI(api_key=open_ai_key)\n",
    "\n",
    "# Load config\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Setup Weaviate connection\n",
    "manager = WeaviateCollectionManager(config=config)\n",
    "manager.connect(connection_type=\"ec2\", host=\"your-ec2-public-ip\")\n",
    "collection = manager.get_collection(\"colqwen\")\n",
    "\n",
    "# Initialize model\n",
    "model = Colqwen(model_name=\"vidore/colqwen2-v1.0\", device_map=\"gpu\", attn_implementation=\"eager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467b65a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae2923",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is the model architecture?\"\n",
    "query_embedding = model.multi_vectorize_text(query_text)\n",
    "\n",
    "image_response = collection.query.near_vector(\n",
    "    near_vector=query_embedding.cpu().float().numpy().tolist(),\n",
    "    target_vector=\"colqwen_vector\",\n",
    "    limit=3,\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For query: {query_text}')\n",
    "\n",
    "returned_images = []\n",
    "for img_obj in image_response.objects:\n",
    "    print('Distance:', img_obj.metadata.distance)\n",
    "\n",
    "    display(load_and_scale_image(img_obj.properties.get('base64_image'), new_height=1024))\n",
    "\n",
    "    returned_images.append(img_obj.properties.get('base64_image'))\n",
    "\n",
    "print(\"##\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3924d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93494f",
   "metadata": {},
   "source": [
    "### Summarizer with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a highly capable multimodal assistant designed to interpret and summarize the content of images with accuracy, clarity, and context-awareness. \n",
    "\n",
    "When given images and a related text query from the user, your goal is to:\n",
    "\n",
    "- Carefully analyze the visual content in each image\n",
    "- Extract key details such as objects, people, text, actions, scenes, or relationships\n",
    "- Connect the visual content with the user's question\n",
    "- Summarize the most important, relevant information without speculation or hallucination\n",
    "- Be clear, concise, and informative â€” using bullet points or paragraphs depending on the context\n",
    "\n",
    "If the user query is vague or open-ended, provide a general but insightful summary of the visual information. If the user query is specific, tailor your response directly to answering their question.\n",
    "\n",
    "Always prioritize factual accuracy. If you are unsure about something in an image, state it with appropriate caution (e.g., \"this appears to be...\").\n",
    "\n",
    "You may be shown multiple images at once. If so, compare or summarize them together if relevant.\n",
    "\n",
    "Respond professionally and helpfully.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09708a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = build_flexible_message_payload(\n",
    "    base64_images=returned_images,\n",
    "    user_text=query_text,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT-4o\n",
    "result = ask_gpt4o(open_ai_client, messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932f453",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe1508",
   "metadata": {},
   "source": [
    "### Non related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71130225",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"How do attention-based strategies improve the outcome of negotiations in business models?\"\n",
    "\n",
    "query_embedding = model.multi_vectorize_text(query_text)\n",
    "\n",
    "image_response = collection.query.near_vector(\n",
    "    near_vector=query_embedding.cpu().float().numpy().tolist(),\n",
    "    target_vector=\"colqwen_vector\",\n",
    "    limit=3,\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b10965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For query: {query_text}')\n",
    "\n",
    "returned_images = []\n",
    "for img_obj in image_response.objects:\n",
    "    print('Distance:', img_obj.metadata.distance)\n",
    "\n",
    "    display(load_and_scale_image(img_obj.properties.get('base64_image'), new_height=1024))\n",
    "\n",
    "    returned_images.append(img_obj.properties.get('base64_image'))\n",
    "\n",
    "print(\"##\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a highly capable multimodal assistant designed to interpret and summarize the content of images with accuracy, clarity, and context-awareness. \n",
    "\n",
    "When given images and a related text query from the user, your goal is to:\n",
    "\n",
    "- Carefully analyze the visual content in each image\n",
    "- Extract key details such as objects, people, text, actions, scenes, or relationships\n",
    "- Connect the visual content with the user's question\n",
    "- Summarize the most important, relevant information without speculation or hallucination\n",
    "- Be clear, concise, and informative â€” using bullet points or paragraphs depending on the context\n",
    "\n",
    "If the user query is vague or open-ended, provide a general but insightful summary of the visual information. If the user query is specific, tailor your response directly to answering their question.\n",
    "\n",
    "Always prioritize factual accuracy. If you are unsure about something in an image, state it with appropriate caution (e.g., \"this appears to be...\").\n",
    "\n",
    "You may be shown multiple images at once. If so, compare or summarize them together if relevant.\n",
    "\n",
    "If the images provided to you are not related to text query from the user, your goal is to:\n",
    "- Say that the provided images are unrelated to the user query before answering\n",
    "- Say that you will provide the best answer based on your general knowledge and not what was given to you\n",
    "\n",
    "Respond professionally and helpfully.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = build_flexible_message_payload(\n",
    "    base64_images=returned_images,\n",
    "    user_text=query_text,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT-4o\n",
    "result = ask_gpt4o(open_ai_client, messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a21a5e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
